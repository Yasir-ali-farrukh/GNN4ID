{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ae7268d",
   "metadata": {},
   "source": [
    "## Data Preprocessing CIC-IoT2023\n",
    "\n",
    "This section covers the preprocessing of the CIC-IoT2023 dataset after extracting its flow-level and packet-level features using the `Feature_extractor_flow_packet_combined.py` script (extraction shown in `GNN4ID.ipynb`) and generation of additional features. Since CIC-IoT2023 dataset is very huge and is very imbalance with having some classes with very low instances, therefore to maintain uniformity we have under and over-sampled data instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2805ec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utility.Functions import *\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4c96a9",
   "metadata": {},
   "source": [
    "For our preprocessing, we divided the data into an 80:20 split. Specifically, 80% of the data is used for training, while the remaining 20% is utilized for testing.\n",
    "\n",
    "To achieve this division, we first identified the class with the least number of samples, which in our scenario was the BruteForce Attack class, with 2,336 samples. Using this as a reference point, we determined the undersampling rate for the other classes based on the number of samples for the minority class.\n",
    "\n",
    "We applied an oversampling factor of 10x to the minority class for the training data, meaning we increased the number of training samples for the BruteForce Attack class (80% of 2,336 samples) to 20,000 samples. Consequently, we limited the number of training samples for each class to 20,000. Depending on the class, we either undersampled or oversampled to achieve this target.\n",
    "\n",
    "The following steps outline the process of dividing the dataset into an 80:20 split and subsequently performing the necessary over/undersampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1b60e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Directory for Extracted flow features+additional features in csv files\n",
    "directory = 'F:/CIC_IOT/Extracted_Flow_Features/'\n",
    "List_of_CSV_File =glob.glob(directory+'*csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60665d1e",
   "metadata": {},
   "source": [
    "### Dataset Division & Filtering\n",
    "\n",
    "For the attack class samples, we are specifically filtering the data by retaining only those flow instances where the attacker's MAC address appears as either the source or destination. This targeted approach ensures that non-attack flows are excluded from the analysis, focusing solely on the relevant attack data.\n",
    "\n",
    "Similarly, we are removing the samples with attacker's MAC address for the Benign Samples so that our data is more clean and filtered.\n",
    "\n",
    "Furthermore, we are capping the number of samples to enhance the efficiency and manageability of the dataset. The test dataset is limited to a maximum of 4,000 samples, while the training dataset is restricted to 20,000 samples. This strategic sampling allows for effective model training and evaluation without compromising on performance or computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96769bc7",
   "metadata": {},
   "source": [
    "The provided code offers full flexibility for personalization, allowing you to process a specific number of files or focus on particular attack classes based on your requirements. Whether you need to analyze just a subset of the data or target specific attack types, the code can be tailored to meet your needs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652f3588",
   "metadata": {},
   "source": [
    "### Test & Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ee8bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering and Spliting Files into Train/Test\n",
    "for files in tqdm(List_of_CSV_File):\n",
    "    split_csv(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8c78fb",
   "metadata": {},
   "source": [
    "##### Combining Sub-Classes into Broad Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c584b210",
   "metadata": {},
   "outputs": [],
   "source": [
    "Attack_Classes = ['Benign', 'BruteForce','DDos','Dos','Mirai','Recon','Spoofing','WebBased']\n",
    "label_dict = {'Benign': 0,'WebBased': 1,'Spoofing': 2,'Recon': 3,'Mirai': 4,'Dos': 5,'DDos': 6,'BruteForce': 7}\n",
    "\n",
    "## Combining Same Class files into one file.\n",
    "Combining_classes(directory,Attack_Classes,label_dict=label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0fcf85",
   "metadata": {},
   "source": [
    "#### Transforming Data into Single Train and Test File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4284f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train_Set\n",
    "directory_combined = directory+'/train/'\n",
    "List_of_CSV_File = glob.glob(directory_combined+\"*\") \n",
    "df_list = []\n",
    "for location in List_of_CSV_File:\n",
    "    df = pd.read_csv(location)\n",
    "    os.remove(location) \n",
    "    df_list.append(df)\n",
    "final_df = pd.concat(df_list, ignore_index=True)\n",
    "final_df.to_csv(directory_combined+'df_class_8_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec2e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some Columns to drop as they contain some biased or highly correlated data. \n",
    "final_df.drop(['src_ip','src_port','dst_ip','dst_port','ip_version','bidirectional_bytes','bidirectional_first_seen_ms','bidirectional_last_seen_ms','bidirectional_duration_ms',\n",
    "         'bidirectional_packets','src2dst_first_seen_ms','src2dst_last_seen_ms','dst2src_first_seen_ms','dst2src_last_seen_ms',\n",
    "         'id','src_mac','src_oui','dst_mac','dst_oui','vlan_id','tunnel_id','bidirectional_syn_packets','bidirectional_cwr_packets',\n",
    "         'bidirectional_ece_packets','bidirectional_urg_packets','bidirectional_ack_packets','bidirectional_psh_packets',\n",
    "         'bidirectional_rst_packets','bidirectional_fin_packets'], axis=1, inplace=True)\n",
    "final_df.to_csv(directory_combined+'df_class_8_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c370d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test_Set\n",
    "directory_combined = directory+'/test/'\n",
    "List_of_CSV_File = glob.glob(directory_combined+\"*\") \n",
    "df_list = []\n",
    "for location in List_of_CSV_File:\n",
    "    df = pd.read_csv(location)\n",
    "    os.remove(location) \n",
    "    df_list.append(df)\n",
    "final_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1106bd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some Columns to drop as they contain some biased or highly correlated data. \n",
    "final_df.drop(['src_ip','src_port','dst_ip','dst_port','ip_version','bidirectional_bytes','bidirectional_first_seen_ms','bidirectional_last_seen_ms','bidirectional_duration_ms',\n",
    "         'bidirectional_packets','src2dst_first_seen_ms','src2dst_last_seen_ms','dst2src_first_seen_ms','dst2src_last_seen_ms',\n",
    "         'id','src_mac','src_oui','dst_mac','dst_oui','vlan_id','tunnel_id','bidirectional_syn_packets','bidirectional_cwr_packets',\n",
    "         'bidirectional_ece_packets','bidirectional_urg_packets','bidirectional_ack_packets','bidirectional_psh_packets',\n",
    "         'bidirectional_rst_packets','bidirectional_fin_packets'], axis=1, inplace=True)\n",
    "final_df.to_csv(directory_combined+'df_class_8_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
